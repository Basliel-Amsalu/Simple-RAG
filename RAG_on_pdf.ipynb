{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Get PDF document path\n",
    "pdf_path = \"KAN.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e495d9041d2442789598f3638c2e6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 1,\n",
       "  'page_char_count': 2287,\n",
       "  'page_word_count': 347,\n",
       "  'page_setence_count_raw': 14,\n",
       "  'page_token_count': 571.75,\n",
       "  'text': 'KAN: Kolmogorov–Arnold Networks Ziming Liu1,4∗ Yixuan Wang2 Sachin Vaidya1 Fabian Ruehle3,4 James Halverson3,4 Marin Soljaˇci´c1,4 Thomas Y. Hou2 Max Tegmark1,4 1 Massachusetts Institute of Technology 2 California Institute of Technology 3 Northeastern University 4 The NSF Institute for Artificial Intelligence and Fundamental Interactions Abstract Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov- Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs). While MLPs have fixed activation functions on nodes (“neurons”), KANs have learnable activation functions on edges (“weights”). KANs have no linear weights at all – every weight parameter is replaced by a univariate function parametrized as a spline. We show that this seemingly simple change makes KANs outperform MLPs in terms of accuracy and interpretability, on small-scale AI + Science tasks. For accuracy, smaller KANs can achieve comparable or better accuracy than larger MLPs in function fitting tasks. Theo- retically and empirically, KANs possess faster neural scaling laws than MLPs. For inter- pretability, KANs can be intuitively visualized and can easily interact with human users. Through two examples in mathematics and physics, KANs are shown to be useful “collabo- rators” helping scientists (re)discover mathematical and physical laws. In summary, KANs are promising alternatives for MLPs, opening opportunities for further improving today’s deep learning models which rely heavily on MLPs. Theorem Formula  (Shallow) Model  (Shallow) Model  (Deep) Multi-Layer Perceptron (MLP) Kolmogorov-Arnold Network (KAN) Universal Approximation Theorem Kolmogorov-Arnold Representation Theorem f(x) ≈ N(ϵ) ∑ i=1 aiσ(wi ⋅ x + bi) f(x) = 2n+1 ∑ q=1 Φq n ∑ p=1 ϕq,p(xp) Model fixed activation functions  on nodes  Formula  (Deep) learnable weights  on edges  learnable activation functions  on edges  sum operation on nodes  MLP(x) = (W3 ∘ σ2 ∘ W2 ∘ σ1 ∘ W1)(x) KAN(x) = (Φ3 ∘ Φ2 ∘ Φ1)(x) W1 σ1 W2 σ2 W3 Φ3 Φ2 Φ1 x x MLP(x) KAN(x) linear,   learnable nonlinear,  fixed nonlinear,  learnable (a) (b) (c) (d) Figure 0.1: Multi-Layer Perceptrons (MLPs) vs. Kolmogorov-Arnold Networks (KANs) ∗zmliu@mit.edu Preprint. Under review. arXiv:2404.19756v3  [cs.LG]  24 May 2024'},\n",
       " {'page_number': 2,\n",
       "  'page_char_count': 3854,\n",
       "  'page_word_count': 571,\n",
       "  'page_setence_count_raw': 25,\n",
       "  'page_token_count': 963.5,\n",
       "  'text': '1 Introduction Multi-layer perceptrons (MLPs) [1, 2, 3], also known as fully-connected feedforward neural net- works, are foundational building blocks of today’s deep learning models. The importance of MLPs can never be overstated, since they are the default models in machine learning for approximating nonlinear functions, due to their expressive power guaranteed by the universal approximation theo- rem [3]. However, are MLPs the best nonlinear regressors we can build? Despite the prevalent use of MLPs, they have significant drawbacks. In transformers [4] for example, MLPs consume almost all non-embedding parameters and are typically less interpretable (relative to attention layers) without post-analysis tools [5]. We propose a promising alternative to MLPs, called Kolmogorov-Arnold Networks (KANs). Whereas MLPs are inspired by the universal approximation theorem, KANs are inspired by the Kolmogorov-Arnold representation theorem [6, 7, 8]. Like MLPs, KANs have fully-connected structures. However, while MLPs place fixed activation functions on nodes (“neurons”), KANs place learnable activation functions on edges (“weights”), as illustrated in Figure 0.1. As a result, KANs have no linear weight matrices at all: instead, each weight parameter is replaced by a learn- able 1D function parametrized as a spline. KANs’ nodes simply sum incoming signals without applying any non-linearities. One might worry that KANs are hopelessly expensive, since each MLP’s weight parameter becomes KAN’s spline function. Fortunately, KANs usually allow much smaller computation graphs than MLPs. Unsurprisingly, the possibility of using Kolmogorov-Arnold representation theorem to build neural networks has been studied [9, 10, 11, 12, 13, 14, 15, 16]. However, most work has stuck with the original depth-2 width-(2n + 1) representation, and many did not have the chance to leverage more modern techniques (e.g., back propagation) to train the networks. In [12], a depth-2 width- (2n + 1) representation was investigated, with breaking of the curse of dimensionality observed both empirically and with an approximation theory given compositional structures of the function. Our contribution lies in generalizing the original Kolmogorov-Arnold representation to arbitrary widths and depths, revitalizing and contextualizing it in today’s deep learning world, as well as using extensive empirical experiments to highlight its potential for AI + Science due to its accuracy and interpretability. Despite their elegant mathematical interpretation, KANs are nothing more than combinations of splines and MLPs, leveraging their respective strengths and avoiding their respective weaknesses. Splines are accurate for low-dimensional functions, easy to adjust locally, and able to switch between different resolutions. However, splines have a serious curse of dimensionality (COD) problem, because of their inability to exploit compositional structures. MLPs, On the other hand, suffer less from COD thanks to their feature learning, but are less accurate than splines in low dimensions, because of their inability to optimize univariate functions. The link between MLPs using ReLU-k as activation functions and splines have been established in [17, 18]. To learn a function accurately, a model should not only learn the compositional structure (external degrees of freedom), but should also approximate well the univariate functions (internal degrees of freedom). KANs are such models since they have MLPs on the outside and splines on the inside. As a result, KANs can not only learn features (thanks to their external similarity to MLPs), but can also optimize these learned features to great accuracy (thanks to their internal similarity to splines). For example, given a high dimensional function f(x1, · · · , xN) = exp   1 N N X i=1 sin2(xi) ! , (1.1) 2'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pdf_path = \"KAN.pdf\"\n",
    "\n",
    "def text_formatter(text: str) -> str: \n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    # Potentially more text formatting functions can go here\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_and_texts = [] \n",
    "    for page_number, page in tqdm(enumerate(doc)):\n",
    "        text = page.get_text()\n",
    "        text = text_formatter(text=text)\n",
    "        pages_and_texts.append({\"page_number\": page_number+1,\n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_setence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4, # 1 token = ~4 characters\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2287</td>\n",
       "      <td>347</td>\n",
       "      <td>14</td>\n",
       "      <td>571.75</td>\n",
       "      <td>KAN: Kolmogorov–Arnold Networks Ziming Liu1,4∗...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3854</td>\n",
       "      <td>571</td>\n",
       "      <td>25</td>\n",
       "      <td>963.50</td>\n",
       "      <td>1 Introduction Multi-layer perceptrons (MLPs) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2916</td>\n",
       "      <td>437</td>\n",
       "      <td>21</td>\n",
       "      <td>729.00</td>\n",
       "      <td>Figure 2.1: Our proposed Kolmogorov-Arnold net...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2735</td>\n",
       "      <td>452</td>\n",
       "      <td>23</td>\n",
       "      <td>683.75</td>\n",
       "      <td>Figure 2.2: Left: Notations of activations tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2754</td>\n",
       "      <td>535</td>\n",
       "      <td>22</td>\n",
       "      <td>688.50</td>\n",
       "      <td>As mentioned, such a network is known to be to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "0            1             2287              347                      14   \n",
       "1            2             3854              571                      25   \n",
       "2            3             2916              437                      21   \n",
       "3            4             2735              452                      23   \n",
       "4            5             2754              535                      22   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0            571.75  KAN: Kolmogorov–Arnold Networks Ziming Liu1,4∗...  \n",
       "1            963.50  1 Introduction Multi-layer perceptrons (MLPs) ...  \n",
       "2            729.00  Figure 2.1: Our proposed Kolmogorov-Arnold net...  \n",
       "3            683.75  Figure 2.2: Left: Notations of activations tha...  \n",
       "4            688.50  As mentioned, such a network is known to be to...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "351798f642c04721babcab4bf40b2315",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "\n",
    "    # Make sure all sentences are strings (the default type is a spaCy datatype)\n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "\n",
    "    # Count the sentences\n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 7,\n",
       "  'page_char_count': 2829,\n",
       "  'page_word_count': 550,\n",
       "  'page_setence_count_raw': 24,\n",
       "  'page_token_count': 707.25,\n",
       "  'text': '(2) with layers of equal width n0 = n1 = · · · = nL = N, (3) with each spline of order k (usually k = 3) on G intervals (for G + 1 grid points). Then there are in total O(N 2L(G + k)) ∼ O(N 2LG) parameters. In contrast, an MLP with depth L and width N only needs O(N 2L) parameters, which appears to be more efficient than KAN. For- tunately, KANs usually require much smaller N than MLPs, which not only saves parameters, but also achieves better generalization (see e.g., Figure 3.1 and 3.3) and facilitates interpretability. We remark that for 1D problems, we can take N = L = 1 and the KAN network in our implementation is nothing but a spline approximation. For higher dimensions, we characterize the generalization behavior of KANs with a theorem below. 2.3 KAN’s Approximation Abilities and Scaling Laws Recall that in Eq. (2.1), the 2-Layer width-(2n + 1) representation may be non-smooth. How- ever, deeper representations may bring the advantages of smoother activations. For example, the 4-variable function f(x1, x2, x3, x4) = exp \\x00sin(x2 1 + x2 2) + sin(x2 3 + x2 4) \\x01 (2.13) can be smoothly represented by a [4, 2, 1, 1] KAN which is 3-Layer, but may not admit a 2-Layer KAN with smooth activations. To facilitate an approximation analysis, we still assume smoothness of activations, but allow the representations to be arbitrarily wide and deep, as in Eq. (2.7). To emphasize the dependence of our KAN on the finite set of grid points, we use ΦG l and ΦG l,i,j below to replace the notation Φl and Φl,i,j used in Eq. (2.5) and (2.6). Theorem 2.1 (Approximation theory, KAT). Let x = (x1, x2, · · · , xn). Suppose that a function f(x) admits a representation f = (ΦL−1 ◦ ΦL−2 ◦ · · · ◦ Φ1 ◦ Φ0)x , (2.14) as in Eq. (2.7), where each one of the Φl,i,j are (k + 1)-times continuously differentiable. Then there exists a constant C depending on f and its representation, such that we have the following approximation bound in terms of the grid size G: there exist k-th order B-spline functions ΦG l,i,j such that for any 0 ≤ m ≤ k, we have the bound ∥f − (ΦG L−1 ◦ ΦG L−2 ◦ · · · ◦ ΦG 1 ◦ ΦG 0 )x∥Cm ≤ CG−k−1+m . (2.15) Here we adopt the notation of Cm-norm measuring the magnitude of derivatives up to order m: ∥g∥Cm = max |β|≤m sup x∈[0,1]n \\x0c\\x0cDβg(x) \\x0c\\x0c . Proof. By the classical 1D B-spline theory [22] and the fact that Φl,i,j as continuous functions can be uniformly bounded on a bounded domain, we know that there exist finite-grid B-spline functions ΦG l,i,j such that for any 0 ≤ m ≤ k, ∥(Φl,i,j ◦Φl−1◦Φl−2◦· · ·◦Φ1◦Φ0)x−(ΦG l,i,j ◦Φl−1◦Φl−2◦· · ·◦Φ1◦Φ0)x∥Cm ≤ CG−k−1+m , with a constant C independent of G. We fix those B-spline approximations. Therefore we have that the residue Rl defined via Rl := (ΦG L−1 ◦ · · · ◦ ΦG l+1 ◦ Φl ◦ Φl−1 ◦ · · · ◦ Φ0)x − (ΦG L−1 ◦ · · · ◦ ΦG l+1 ◦ ΦG l ◦ Φl−1 ◦ · · · ◦ Φ0)x 7',\n",
       "  'sentences': ['(2) with layers of equal width n0 = n1 = · · · = nL = N, (3) with each spline of order k (usually k = 3) on G intervals (for G + 1 grid points).',\n",
       "   'Then there are in total O(N 2L(G + k)) ∼ O(N 2LG) parameters.',\n",
       "   'In contrast, an MLP with depth L and width N only needs O(N 2L) parameters, which appears to be more efficient than KAN.',\n",
       "   'For- tunately, KANs usually require much smaller N than MLPs, which not only saves parameters, but also achieves better generalization (see e.g., Figure 3.1 and 3.3) and facilitates interpretability.',\n",
       "   'We remark that for 1D problems, we can take N = L = 1 and the KAN network in our implementation is nothing but a spline approximation.',\n",
       "   'For higher dimensions, we characterize the generalization behavior of KANs with a theorem below.',\n",
       "   '2.3 KAN’s Approximation Abilities and Scaling Laws Recall that in Eq. (',\n",
       "   '2.1), the 2-Layer width-(2n + 1) representation may be non-smooth.',\n",
       "   'How- ever, deeper representations may bring the advantages of smoother activations.',\n",
       "   'For example, the 4-variable function f(x1, x2, x3, x4) = exp \\x00sin(x2 1 + x2 2) + sin(x2 3 + x2 4) \\x01 (2.13) can be smoothly represented by a [4, 2, 1, 1] KAN which is 3-Layer, but may not admit a 2-Layer KAN with smooth activations.',\n",
       "   'To facilitate an approximation analysis, we still assume smoothness of activations, but allow the representations to be arbitrarily wide and deep, as in Eq. (',\n",
       "   '2.7).',\n",
       "   'To emphasize the dependence of our KAN on the finite set of grid points, we use ΦG l and ΦG l,i,j below to replace the notation Φl and Φl,i,j used in Eq. (',\n",
       "   '2.5) and (2.6).',\n",
       "   'Theorem 2.1 (Approximation theory, KAT).',\n",
       "   'Let x = (x1, x2, · · · , xn).',\n",
       "   'Suppose that a function f(x) admits a representation f = (ΦL−1 ◦ ΦL−2 ◦ · · · ◦ Φ1 ◦ Φ0)x , (2.14) as in Eq. (',\n",
       "   '2.7), where each one of the Φl,i,j are (k + 1)-times continuously differentiable.',\n",
       "   'Then there exists a constant C depending on f and its representation, such that we have the following approximation bound in terms of the grid size G: there exist k-th order B-spline functions ΦG l,i,j such that for any 0 ≤ m ≤ k, we have the bound ∥f − (ΦG L−1 ◦ ΦG L−2 ◦ · · · ◦ ΦG 1 ◦ ΦG 0 )x∥Cm ≤ CG−k−1+m . (',\n",
       "   '2.15) Here we adopt the notation of Cm-norm measuring the magnitude of derivatives up to order m: ∥g∥Cm = max |β|≤m sup x∈[0,1]n \\x0c\\x0cDβg(x) \\x0c\\x0c .',\n",
       "   'Proof.',\n",
       "   'By the classical 1D B-spline theory [22] and the fact that Φl,i,j as continuous functions can be uniformly bounded on a bounded domain, we know that there exist finite-grid B-spline functions ΦG l,i,j such that for any 0 ≤ m ≤ k, ∥(Φl,i,j ◦Φl−1◦Φl−2◦· · ·◦Φ1◦Φ0)x−(ΦG l,i,j ◦Φl−1◦Φl−2◦· · ·◦Φ1◦Φ0)x∥Cm ≤ CG−k−1+m , with a constant C independent of G. We fix those B-spline approximations.',\n",
       "   'Therefore we have that the residue Rl defined via Rl := (ΦG L−1 ◦ · · · ◦ ΦG l+1 ◦ Φl ◦ Φl−1 ◦ · · · ◦ Φ0)x − (ΦG L−1 ◦ · · · ◦ ΦG l+1 ◦ ΦG l ◦ Φl−1 ◦ · · · ◦ Φ0)x 7'],\n",
       "  'page_sentence_count_spacy': 23}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample(pages_and_texts, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_setence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "      <td>50.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>25.50</td>\n",
       "      <td>2692.48</td>\n",
       "      <td>446.52</td>\n",
       "      <td>24.72</td>\n",
       "      <td>673.12</td>\n",
       "      <td>24.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.58</td>\n",
       "      <td>1242.93</td>\n",
       "      <td>228.45</td>\n",
       "      <td>14.89</td>\n",
       "      <td>310.73</td>\n",
       "      <td>14.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>32.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.25</td>\n",
       "      <td>2184.25</td>\n",
       "      <td>369.50</td>\n",
       "      <td>19.00</td>\n",
       "      <td>546.06</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.50</td>\n",
       "      <td>2886.00</td>\n",
       "      <td>444.50</td>\n",
       "      <td>23.00</td>\n",
       "      <td>721.50</td>\n",
       "      <td>23.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.75</td>\n",
       "      <td>3346.00</td>\n",
       "      <td>567.75</td>\n",
       "      <td>28.00</td>\n",
       "      <td>836.50</td>\n",
       "      <td>28.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>50.00</td>\n",
       "      <td>6578.00</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>64.00</td>\n",
       "      <td>1644.50</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       page_number  page_char_count  page_word_count  page_setence_count_raw  \\\n",
       "count        50.00            50.00            50.00                   50.00   \n",
       "mean         25.50          2692.48           446.52                   24.72   \n",
       "std          14.58          1242.93           228.45                   14.89   \n",
       "min           1.00            32.00             6.00                    1.00   \n",
       "25%          13.25          2184.25           369.50                   19.00   \n",
       "50%          25.50          2886.00           444.50                   23.00   \n",
       "75%          37.75          3346.00           567.75                   28.00   \n",
       "max          50.00          6578.00          1302.00                   64.00   \n",
       "\n",
       "       page_token_count  page_sentence_count_spacy  \n",
       "count             50.00                      50.00  \n",
       "mean             673.12                      24.02  \n",
       "std              310.73                      14.47  \n",
       "min                8.00                       1.00  \n",
       "25%              546.06                      19.00  \n",
       "50%              721.50                      23.00  \n",
       "75%              836.50                      28.00  \n",
       "max             1644.50                      61.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.describe().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_chunk_list_size = 10\n",
    "\n",
    "def chunk_text(input_text: list[str], chunk_size: int= num_chunk_list_size) -> list[list[str]]:\n",
    "    return [input_text[i: i+chunk_size] for i in range(0, len(input_text), chunk_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de581d97b30740d481c052fbb3e2909c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = chunk_text(item[\"sentences\"])\n",
    "    item[\"sentence_chunks_num\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Figure C.1: Effects of hyperparameters on interpretability results.',\n",
       "  'E Remark on grid size For both PDE and regression tasks, when we choose the training data on uniform grids, we witness a sudden increase in training loss (i.e., sudden drop in performance) when the grid size is updated to a large level, comparable to the different training points in one spatial direction.',\n",
       "  'This could be due to implementation of B-spline in higher dimensions and needs further investigation.',\n",
       "  'F KANs for special functions We include more results on the special function dataset (Section 3.2).',\n",
       "  'Figure F.2 and F.1 visualize minimal KANs (under the constraint test RMSE < 10−2) and best KANs (with the lowest test RMSE loss) for each special function fitting task.',\n",
       "  '45']]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd = random.sample(pages_and_texts, k=1)\n",
    "rnd_chunk = rnd[0][\"sentence_chunks\"]\n",
    "rnd_chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
